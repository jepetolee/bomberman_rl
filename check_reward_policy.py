#!/usr/bin/env python3
"""
ë³´ìƒ ì •ì±… í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
=====================

í•™ìŠµëœ ëª¨ë¸ì´ ì–´ë–¤ ë³´ìƒ ì •ì±…ì„ ë”°ë¥´ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.

1. ëª¨ë¸ì˜ í–‰ë™ ë¶„í¬ ë¶„ì„
2. ë³´ìƒ í•¨ìˆ˜ì™€ì˜ ì¼ì¹˜ë„ í™•ì¸
3. ì „ëµ íŒ¨í„´ ë¶„ì„ (ê³µê²©ì /ë°©ì–´ì /ìˆ˜ì§‘ ì¤‘ì‹¬)

Usage:
    python check_reward_policy.py --model-path ppo_model.pt --rounds 100
"""

import os
import sys
import json
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict, Counter

import numpy as np
import torch
import torch.nn as nn

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from agent_code.ppo_agent.models.vit_trm import PolicyValueViT_TRM_Hybrid
from config.load_config import load_config, get_model_config
from agent_code.ppo_agent.features import state_to_features


ACTIONS = ['UP', 'RIGHT', 'DOWN', 'LEFT', 'WAIT', 'BOMB']
ACTION_TO_IDX = {a: i for i, a in enumerate(ACTIONS)}


def load_model(model_path: str, device: torch.device) -> nn.Module:
    """ëª¨ë¸ ë¡œë“œ"""
    config = load_config()
    model_config = get_model_config(config)
    
    model = PolicyValueViT_TRM_Hybrid(
        in_channels=model_config.get('in_channels', 11),
        num_actions=model_config.get('num_actions', 6),
        img_size=tuple(model_config.get('img_size', [17, 17])),
        embed_dim=model_config.get('embed_dim', 256),
        vit_depth=model_config.get('vit_depth', 2),
        vit_heads=model_config.get('vit_heads', 4),
        vit_mlp_ratio=model_config.get('vit_mlp_ratio', 4.0),
        vit_patch_size=model_config.get('vit_patch_size', 1),
        trm_n_latent=model_config.get('trm_n_latent', 4),
        trm_mlp_ratio=model_config.get('trm_mlp_ratio', 4.0),
        trm_drop=model_config.get('trm_drop', 0.0),
        trm_patch_size=model_config.get('trm_patch_size', 2),
        trm_patch_stride=model_config.get('trm_patch_stride', 1),
        use_ema=model_config.get('use_ema', True),
        ema_decay=model_config.get('ema_decay', 0.999),
    ).to(device)
    
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ: {model_path}")
    else:
        print(f"âœ— ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        sys.exit(1)
    
    model.eval()
    return model


def analyze_action_distribution(
    model: nn.Module,
    num_samples: int = 1000,
    device: torch.device = None
) -> Dict:
    """ëª¨ë¸ì˜ í–‰ë™ ë¶„í¬ ë¶„ì„"""
    if device is None:
        device = next(model.parameters()).device
    
    # ëœë¤ ìƒíƒœ ìƒì„± (ì‹¤ì œ ê²Œì„ ìƒíƒœì™€ ìœ ì‚¬í•˜ê²Œ)
    action_counts = Counter()
    action_probs_sum = np.zeros(6)
    value_sum = 0.0
    value_count = 0
    
    print(f"í–‰ë™ ë¶„í¬ ë¶„ì„ ì¤‘... ({num_samples} ìƒ˜í”Œ)")
    
    with torch.no_grad():
        for i in range(num_samples):
            # ëœë¤ ìƒíƒœ ìƒì„± (10 channels, 17x17)
            random_state = torch.randn(1, 10, 17, 17).to(device)
            
            # Forward pass
            if hasattr(model, 'forward_with_z'):
                logits, value, _ = model.forward_with_z(random_state, z_prev=None)
            else:
                logits, value = model(random_state)
            
            # í–‰ë™ ì„ íƒ (í™•ë¥  ë¶„í¬ì—ì„œ ìƒ˜í”Œë§)
            probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
            action_probs_sum += probs
            
            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í–‰ë™
            action_idx = int(torch.argmax(logits, dim=-1).item())
            action_counts[ACTIONS[action_idx]] += 1
            
            value_sum += float(value.item())
            value_count += 1
    
    avg_probs = action_probs_sum / num_samples
    avg_value = value_sum / value_count if value_count > 0 else 0.0
    
    return {
        'action_counts': dict(action_counts),
        'action_probs': {ACTIONS[i]: float(avg_probs[i]) for i in range(6)},
        'avg_value': avg_value,
    }


def check_reward_alignment(
    model_path: str,
    rounds: int = 50
) -> Dict:
    """ì‹¤ì œ ê²Œì„ì—ì„œ ë³´ìƒ ì •ì±…ê³¼ì˜ ì¼ì¹˜ë„ í™•ì¸"""
    print(f"\nì‹¤ì œ ê²Œì„ í‰ê°€ ì¤‘... ({rounds} ë¼ìš´ë“œ)")
    
    # í‰ê°€ ì‹¤í–‰
    eval_dir = "results/reward_policy_check"
    os.makedirs(eval_dir, exist_ok=True)
    
    output_file = os.path.join(eval_dir, "policy_check.json")
    
    cmd = [
        sys.executable, 'main.py', 'play',
        '--agents', 'ppo_agent', 'ppo_agent', 'random_agent', 'random_agent',
        '--no-gui',
        '--n-rounds', str(rounds),
        '--save-stats', output_file,
        '--silence-errors',
    ]
    
    env = os.environ.copy()
    env['PPO_MODEL_PATH'] = model_path
    env['BOMBER_USE_TRM'] = '1'
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300,
            env=env,
            cwd=os.path.dirname(os.path.abspath(__file__))
        )
        
        if result.returncode != 0 or not os.path.exists(output_file):
            print("âœ— ê²Œì„ ì‹¤í–‰ ì‹¤íŒ¨")
            return None
        
        with open(output_file, 'r') as f:
            data = json.load(f)
        
        # PPO ì—ì´ì „íŠ¸ í†µê³„ ì¶”ì¶œ
        ppo_stats = {}
        total_kills = 0
        total_deaths = 0
        total_coins = 0
        total_bombs = 0
        
        by_agent = data.get('by_agent', {})
        for agent_name, stats in by_agent.items():
            if agent_name.startswith('ppo_agent'):
                total_kills += stats.get('kills', 0)
                total_deaths += stats.get('suicides', 0) + stats.get('got_killed', 0)
                total_coins += stats.get('coins_collected', 0)
                total_bombs += stats.get('bombs_dropped', 0)
        
        ppo_stats = {
            'kills': total_kills,
            'deaths': total_deaths,
            'coins': total_coins,
            'bombs': total_bombs,
            'kill_death_ratio': total_kills / max(total_deaths, 1),
        }
        
        return ppo_stats
        
    except Exception as e:
        print(f"âœ— í‰ê°€ ì‹¤íŒ¨: {e}")
        return None


def analyze_strategy(action_dist: Dict, game_stats: Dict = None) -> Dict:
    """ì „ëµ íŒ¨í„´ ë¶„ì„"""
    strategy = {
        'aggressive': False,  # í‚¬ ì¤‘ì‹¬
        'defensive': False,   # ìƒì¡´ ì¤‘ì‹¬
        'collector': False,   # ì½”ì¸ ìˆ˜ì§‘ ì¤‘ì‹¬
        'bomber': False,      # í­íƒ„ ì‚¬ìš© ì¤‘ì‹¬
    }
    
    # í–‰ë™ ë¶„í¬ ê¸°ë°˜ ë¶„ì„
    bomb_prob = action_dist['action_probs'].get('BOMB', 0.0)
    wait_prob = action_dist['action_probs'].get('WAIT', 0.0)
    move_probs = sum(action_dist['action_probs'].get(a, 0.0) for a in ['UP', 'DOWN', 'LEFT', 'RIGHT'])
    
    if bomb_prob > 0.15:  # 15% ì´ìƒ í­íƒ„ ì‚¬ìš©
        strategy['bomber'] = True
        strategy['aggressive'] = True
    
    if wait_prob < 0.1:  # ëŒ€ê¸° ì ìŒ
        strategy['aggressive'] = True
    
    if move_probs > 0.6:  # ì´ë™ ë¹ˆë²ˆ
        strategy['collector'] = True
    
    # ê²Œì„ í†µê³„ ê¸°ë°˜ ë¶„ì„
    if game_stats:
        kdr = game_stats.get('kill_death_ratio', 0.0)
        if kdr > 1.5:
            strategy['aggressive'] = True
        elif kdr < 0.5:
            strategy['defensive'] = True
        
        if game_stats.get('coins', 0) > 50:  # ë§ì€ ì½”ì¸ ìˆ˜ì§‘
            strategy['collector'] = True
    
    return strategy


def print_report(
    action_dist: Dict,
    game_stats: Dict = None,
    strategy: Dict = None
):
    """ë³´ê³ ì„œ ì¶œë ¥"""
    print("\n" + "="*70)
    print("ë³´ìƒ ì •ì±… ë¶„ì„ ë³´ê³ ì„œ")
    print("="*70)
    
    print("\nğŸ“Š í–‰ë™ ë¶„í¬:")
    print("-" * 70)
    for action, prob in sorted(action_dist['action_probs'].items(), key=lambda x: x[1], reverse=True):
        bar_length = int(prob * 50)
        bar = "â–ˆ" * bar_length
        print(f"  {action:<6} {prob*100:>5.1f}% {bar}")
    
    print(f"\n  í‰ê·  ê°€ì¹˜ ì˜ˆì¸¡: {action_dist['avg_value']:.2f}")
    
    if game_stats:
        print("\nğŸ® ê²Œì„ í†µê³„:")
        print("-" * 70)
        print(f"  í‚¬: {game_stats['kills']}")
        print(f"  ì‚¬ë§: {game_stats['deaths']}")
        print(f"  í‚¬/ì‚¬ë§ ë¹„ìœ¨: {game_stats['kill_death_ratio']:.2f}")
        print(f"  ì½”ì¸ ìˆ˜ì§‘: {game_stats['coins']}")
        print(f"  í­íƒ„ ì‚¬ìš©: {game_stats['bombs']}")
    
    if strategy:
        print("\nğŸ¯ ì „ëµ íŒ¨í„´:")
        print("-" * 70)
        if strategy['aggressive']:
            print("  âœ“ ê³µê²©ì  ì „ëµ (í‚¬ ì¤‘ì‹¬)")
        if strategy['defensive']:
            print("  âœ“ ë°©ì–´ì  ì „ëµ (ìƒì¡´ ì¤‘ì‹¬)")
        if strategy['collector']:
            print("  âœ“ ìˆ˜ì§‘ ì¤‘ì‹¬ ì „ëµ")
        if strategy['bomber']:
            print("  âœ“ í­íƒ„ í™œìš© ì „ëµ")
        
        if not any(strategy.values()):
            print("  âš  ëª…í™•í•œ ì „ëµ íŒ¨í„´ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ")
    
    print("\n" + "="*70)


def main():
    parser = argparse.ArgumentParser(description='ë³´ìƒ ì •ì±… í™•ì¸')
    parser.add_argument('--model-path', type=str, default='ppo_model.pt', help='ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--rounds', type=int, default=50, help='í‰ê°€ ë¼ìš´ë“œ ìˆ˜')
    parser.add_argument('--samples', type=int, default=1000, help='í–‰ë™ ë¶„í¬ ë¶„ì„ ìƒ˜í”Œ ìˆ˜')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # ëª¨ë¸ ë¡œë“œ
    model = load_model(args.model_path, device)
    
    # í–‰ë™ ë¶„í¬ ë¶„ì„
    action_dist = analyze_action_distribution(model, num_samples=args.samples, device=device)
    
    # ì‹¤ì œ ê²Œì„ í‰ê°€
    game_stats = check_reward_alignment(args.model_path, rounds=args.rounds)
    
    # ì „ëµ ë¶„ì„
    strategy = analyze_strategy(action_dist, game_stats)
    
    # ë³´ê³ ì„œ ì¶œë ¥
    print_report(action_dist, game_stats, strategy)


if __name__ == '__main__':
    main()

