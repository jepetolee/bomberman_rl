# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê°€ì´ë“œ

ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ì„ ì‹¤í–‰í•˜ê³ , ë³´ìƒ ì •ì±…ì„ í™•ì¸í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

## ğŸ“‹ ì „ì²´ í”„ë¡œì„¸ìŠ¤

```
1. ì‚¬ì „ í•™ìŠµ ëª¨ë¸ í™•ì¸
   â†“
2. ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ ì‹¤í–‰ (A3C)
   â†“
3. ë³´ìƒ ì •ì±… í™•ì¸ ë° ë¶„ì„
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ë°©ë²• 1: ìë™ ìŠ¤í¬ë¦½íŠ¸ (ê¶Œì¥)

```bash
# ê¸°ë³¸ ì‹¤í–‰ (ppo_model.pt ì‚¬ìš©)
./run_full_pipeline.sh

# ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì§€ì •
./run_full_pipeline.sh /path/to/pretrained_model.pt

# ëª¨ë“  ì˜µì…˜ ì§€ì •
./run_full_pipeline.sh \
  ppo_model.pt \                    # ì‚¬ì „ í•™ìŠµ ëª¨ë¸
  config/trm_config.yaml \          # ì„¤ì • íŒŒì¼
  results/curriculum \              # ê²°ê³¼ ë””ë ‰í† ë¦¬
  4 \                                # ì›Œì»¤ ìˆ˜
  50000                              # ì´ ë¼ìš´ë“œ
```

### ë°©ë²• 2: ë‹¨ê³„ë³„ ì‹¤í–‰

#### 1ë‹¨ê³„: ì‚¬ì „ í•™ìŠµ ëª¨ë¸ í™•ì¸

```bash
# ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
ls -lh ppo_model.pt

# ëª¨ë¸ ì •ë³´ í™•ì¸
python3 -c "
import torch
state_dict = torch.load('ppo_model.pt', map_location='cpu', weights_only=True)
print(f'íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in state_dict.values()):,}')
print(f'ë ˆì´ì–´ ìˆ˜: {len(state_dict)}')
"
```

**ëª¨ë¸ì´ ì—†ìœ¼ë©´:**
```bash
# Phase 2 ì‚¬ì „ í•™ìŠµ ì‹¤í–‰
python3 train_phase2.py --train-policy --num-epochs 100 --batch-size 512
```

#### 2ë‹¨ê³„: ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ ì‹¤í–‰

```bash
# A3C ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ (ê¸°ë³¸)
python3 a3c_gpu_train.py \
  --num-workers 4 \
  --total-rounds 50000 \
  --rounds-per-batch 5 \
  --sync-interval 40 \
  --results-dir results/curriculum \
  --model-path ppo_model.pt

# ë˜ëŠ” Planning í¬í•¨ (í™˜ê²½ ëª¨ë¸ í•„ìš”)
python3 a3c_planning_train.py \
  --num-workers 4 \
  --total-rounds 50000 \
  --planning-steps 100 \
  --env-model-path data/env_models/env_model.pt \
  --model-path ppo_model.pt
```

**ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„:**
- Stage 1: Easy (random_agent, peaceful_agent) - ìŠ¹ë¥  60% ì´ìƒ
- Stage 2: Medium (peaceful_agent, coin_collector_agent) - ìŠ¹ë¥  65% ì´ìƒ
- Stage 3: Hard (coin_collector_agent, rule_based_agent) - ìŠ¹ë¥  70% ì´ìƒ
- Stage 4: Expert (team_teacher_agent, aggressive_teacher_agent) - ìŠ¹ë¥  75% ì´ìƒ
- Stage 5: Self-Play (ê°™ì€ ëª¨ë¸ë¼ë¦¬ ëŒ€ì „)

#### 3ë‹¨ê³„: ë³´ìƒ ì •ì±… í™•ì¸

```bash
# ë³´ìƒ ì •ì±… ë¶„ì„
python3 check_reward_policy.py \
  --model-path ppo_model.pt \
  --rounds 100 \
  --samples 1000

# ì „ì²´ í‰ê°€ (ë‹¤ì–‘í•œ ìƒëŒ€ì™€ ëŒ€ì „)
python3 evaluate_model.py \
  --model-path ppo_model.pt \
  --rounds 50 \
  --results-dir results/evaluation
```

---

## ğŸ“Š ë³´ìƒ ì •ì±… ë¶„ì„ ê²°ê³¼ í•´ì„

### í–‰ë™ ë¶„í¬ ë¶„ì„

```
í–‰ë™ ë¶„í¬:
  BOMB    25.3% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  RIGHT   18.7% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  UP      16.2% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  DOWN    15.8% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  LEFT    14.1% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  WAIT    9.9%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

**ì˜ë¯¸:**
- **BOMB ë¹„ìœ¨ì´ ë†’ìŒ (25%)**: ê³µê²©ì  ì „ëµ, í‚¬ ì¤‘ì‹¬
- **WAIT ë¹„ìœ¨ì´ ë‚®ìŒ (<10%)**: ìˆ˜ë™ì  í–‰ë™ ìµœì†Œí™”
- **ì´ë™ í–‰ë™ ê· í˜•**: ì „ëµì  ìœ„ì¹˜ ì´ë™

### ê²Œì„ í†µê³„ ë¶„ì„

```
ê²Œì„ í†µê³„:
  í‚¬: 45
  ì‚¬ë§: 12
  í‚¬/ì‚¬ë§ ë¹„ìœ¨: 3.75
  ì½”ì¸ ìˆ˜ì§‘: 23
  í­íƒ„ ì‚¬ìš©: 67
```

**ì˜ë¯¸:**
- **í‚¬/ì‚¬ë§ ë¹„ìœ¨ > 1.5**: ê³µê²©ì  ì „ëµ ì„±ê³µ
- **í­íƒ„ ì‚¬ìš© ë§ìŒ**: ì ê·¹ì ì¸ ì „íˆ¬ ì°¸ì—¬
- **ì½”ì¸ ìˆ˜ì§‘**: ë³´ì¡° ëª©í‘œë„ ìˆ˜í–‰

### ì „ëµ íŒ¨í„´

```
ì „ëµ íŒ¨í„´:
  âœ“ ê³µê²©ì  ì „ëµ (í‚¬ ì¤‘ì‹¬)
  âœ“ í­íƒ„ í™œìš© ì „ëµ
  âœ“ ìˆ˜ì§‘ ì¤‘ì‹¬ ì „ëµ
```

**ì˜ë¯¸:**
- **ê³µê²©ì  ì „ëµ**: í‚¬ì„ ìš°ì„ ì‹œí•˜ëŠ” í–‰ë™
- **í­íƒ„ í™œìš©**: ì „íˆ¬ì—ì„œ í­íƒ„ ì ê·¹ ì‚¬ìš©
- **ìˆ˜ì§‘ ì¤‘ì‹¬**: ì½”ì¸ ìˆ˜ì§‘ë„ ë³‘í–‰

---

## âš™ï¸ ê³ ê¸‰ ì„¤ì •

### ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ íŒŒë¼ë¯¸í„° ì¡°ì •

```bash
# ë” ë¹ ë¥¸ í•™ìŠµ (ì ì€ ë¼ìš´ë“œ)
python3 a3c_gpu_train.py \
  --num-workers 8 \
  --total-rounds 20000 \
  --rounds-per-batch 10 \
  --sync-interval 20

# ë” ì•ˆì •ì ì¸ í•™ìŠµ (ë§ì€ ë¼ìš´ë“œ)
python3 a3c_gpu_train.py \
  --num-workers 4 \
  --total-rounds 100000 \
  --rounds-per-batch 5 \
  --sync-interval 50
```

### ë³´ìƒ ì •ì±… ìƒì„¸ ë¶„ì„

```bash
# ë” ë§ì€ ìƒ˜í”Œë¡œ ì •í™•í•œ ë¶„ì„
python3 check_reward_policy.py \
  --model-path ppo_model.pt \
  --rounds 200 \
  --samples 5000

# íŠ¹ì • ìƒëŒ€ì™€ë§Œ í‰ê°€
python3 evaluate_model.py \
  --model-path ppo_model.pt \
  --opponents aggressive_teacher_agent,rule_based_agent \
  --rounds 100
```

---

## ğŸ“ ê²°ê³¼ íŒŒì¼ êµ¬ì¡°

```
results/
â”œâ”€â”€ curriculum/                    # ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ ê²°ê³¼
â”‚   â”œâ”€â”€ curriculum_training.log   # í•™ìŠµ ë¡œê·¸
â”‚   â”œâ”€â”€ ppo_model.pt              # ìµœì¢… ëª¨ë¸
â”‚   â””â”€â”€ worker_*.json             # ì›Œì»¤ë³„ í†µê³„
â”‚
â”œâ”€â”€ evaluation/                    # í‰ê°€ ê²°ê³¼
â”‚   â”œâ”€â”€ evaluation_summary.json   # í‰ê°€ ìš”ì•½
â”‚   â””â”€â”€ eval_*.json               # ìƒëŒ€ë³„ ìƒì„¸ ê²°ê³¼
â”‚
â””â”€â”€ reward_policy_check/           # ë³´ìƒ ì •ì±… ë¶„ì„
    â””â”€â”€ policy_check.json         # ì •ì±… ë¶„ì„ ê²°ê³¼
```

---

## ğŸ” ë¬¸ì œ í•´ê²°

### ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ

```bash
# Phase 2 ì‚¬ì „ í•™ìŠµ ì‹¤í–‰
python3 train_phase2.py --train-policy --num-epochs 100

# ë˜ëŠ” ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œ í™•ì¸
find . -name "*.pt" -type f
```

### CUDA Out of Memory

```bash
# ì›Œì»¤ ìˆ˜ ì¤„ì´ê¸°
python3 a3c_gpu_train.py --num-workers 2

# ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python3 train_phase2.py --batch-size 256
```

### ì»¤ë¦¬í˜ëŸ¼ì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ

```bash
# ìŠ¹ë¥  í™•ì¸
grep "win_rate" results/curriculum/curriculum_training.log

# ë” ë§ì€ ë¼ìš´ë“œ ì‹¤í–‰
python3 a3c_gpu_train.py --total-rounds 100000
```

---

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```bash
# í•™ìŠµ ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
tail -f results/curriculum/curriculum_training.log

# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
watch -n 1 nvidia-smi
```

### ì„±ëŠ¥ ì§€í‘œ

- **ìŠ¹ë¥ **: ê° ë‹¨ê³„ë³„ ìŠ¹ë¥  (60% â†’ 65% â†’ 70% â†’ 75%)
- **í‚¬/ì‚¬ë§ ë¹„ìœ¨**: ê³µê²© íš¨ìœ¨ì„± (ëª©í‘œ: > 2.0)
- **ì½”ì¸ ìˆ˜ì§‘**: ë³´ì¡° ëª©í‘œ ë‹¬ì„±ë„
- **í‰ê·  ì ìˆ˜**: ì „ì²´ ì„±ëŠ¥ ì§€í‘œ

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **Self-Play ê°•í™”**: Stage 5ì—ì„œ ë” ë§ì€ ë¼ìš´ë“œ ì‹¤í–‰
2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ì¡°ì •
3. **ëª¨ë¸ ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
4. **ì „ë¬¸ê°€ ìƒëŒ€**: ë” ê°•í•œ ìƒëŒ€ì™€ ëŒ€ì „í•˜ì—¬ ì‹¤ë ¥ í–¥ìƒ

---

## ğŸ“ ìš”ì•½

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ í•œ ë²ˆì— ì‹¤í–‰
./run_full_pipeline.sh ppo_model.pt

# ë˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰
python3 a3c_gpu_train.py --num-workers 4 --total-rounds 50000
python3 check_reward_policy.py --model-path ppo_model.pt --rounds 100
python3 evaluate_model.py --model-path ppo_model.pt --rounds 50
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹: 4-8ì‹œê°„ (ì›Œì»¤ ìˆ˜ì™€ ë¼ìš´ë“œì— ë”°ë¼)
- ë³´ìƒ ì •ì±… í™•ì¸: 5-10ë¶„
- ì „ì²´ í‰ê°€: 10-20ë¶„

