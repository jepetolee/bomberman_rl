# TRM (Tiny Recursive Model) Configuration
# ==========================================
# This configuration file defines all hyperparameters for TRM-based training

# Model Architecture
# ------------------
model:
  type: "recursive_gtrxl"  # "trm" (ViT+TRM hybrid) or "efficient_gtrxl"
  
  # Vision Transformer base settings
  in_channels: 10  # Must match state_to_features output
  num_actions: 6
  img_size: [17, 17]  # [height, width]
  embed_dim: 256      # Shared embed dim for ViT & TRM
  
  # TRM-specific settings (DISABLED - not used)
  trm:
    enabled: true
    n_latent: 4
    n_sup: 8
    T: 3
    use_ema: true
    ema_decay: 0.999
  
  # Patch embedding settings
  patch:
    size: 1                # Patch size (kernel_size)
    stride: 1              # Patch stride
  
  # MLP settings
  mlp_ratio: 4.0           # MLP hidden size = embed_dim * mlp_ratio
  dropout: 0.0             # Dropout rate
  
  # Standard ViT settings (Enhanced for better performance)
  vit:
    depth: 2               # Increased depth (was 2, now 6)
    num_heads: 4           # Increased heads (was 4, now 8)
    mixer: "attn"          # "attn" or "gdn"
    use_cls_token: false
  
  # EfficientNetB0 + GTrXL settings
  efficient_gtrxl:
    # CNN backbone parameters
    cnn_base_channels: 32  # Base channels for EfficientNet
    cnn_width_mult: 1.0    # Width multiplier (like EfficientNet scaling)
    # GTrXL parameters
    gtrxl_depth: 4         # Number of GTrXL blocks
    num_heads: 8           # Number of attention heads
    memory_size: 128       # Memory length for Transformer-XL
  
  # Recursive GTrXL: "One Block, Many Thoughts" (Parameter-Efficient Deep Reasoning)
  recursive_gtrxl:
    # CNN backbone parameters
    cnn_base_channels: 32  # Base channels for EfficientNet
    cnn_width_mult: 1.0    # Width multiplier (like EfficientNet scaling)
    # Recursive GTrXL parameters
    n_layers_simulated: 4  # Number of recursive passes (K) - 추론 깊이
                            # 4~8 권장, 작게 시작할 때는 4로 시작
    num_heads: 8           # Number of attention heads
    memory_size: 128       # Memory length for Transformer-XL
                            # 각 재귀 단계마다 별도 메모리 슬롯 사용


# Phase 1: Teacher Data Collection
# ---------------------------------
phase1:
  episodes: 20000          # Total episodes to collect
  workers: 4               # Number of parallel workers
  data_dir: "data/teacher_episodes"
  min_episode_length: 10   # Minimum steps for valid episode
  
  # Opponents for data collection
  opponents:
    - "random_agent"
    - "peaceful_agent"
    - "coin_collector_agent"
    - "rule_based_agent"
    - "team_teacher_agent"
    - "aggressive_teacher_agent"
  
  teacher_agent: "aggressive_teacher_agent"


# Phase 2: Dyna-Q Planning + DeepSupervision
# -------------------------------------------
phase2:
  # Environment Model Training
  env_model:
    enabled: true
    num_epochs: 50
    batch_size: 128
    learning_rate: 0.001
    model_path: "data/env_models/env_model.pt"
    
    # Architecture
    hidden_dim: 256
    use_deterministic: true  # Deterministic vs stochastic transitions
  
  # Dyna-Q Planning
  planning:
    enabled: true
    n_planning_steps: 100    # Number of planning steps per epoch
    visited_states_buffer_size: 10000  # Max visited states to store
    batch_size: 32           # Batch size for parallel planning
  
  # DeepSupervision Training
  deepsupervision:
    enabled: true
    num_epochs: 50        # More epochs for large model
    batch_size: 512        # Increased batch size (RTX 4090 has plenty of VRAM)
    learning_rate: 1e-4    # Lower LR for very large model (256-dim)
    
    # Strategy: "last" (paper), "all" (average), "weighted" (weighted average)
    strategy: "all"
    
    # Value network supervision
    train_value: true              # Train value network with rewards
    value_weight: 0.5              # Weight for value loss relative to policy loss
    
    # Experience weights
    real_experience_weight: 1.0    # Weight for real experiences
    sim_experience_weight: 0.5     # Weight for simulated experiences
    
    # Policy model path
    policy_model_path: "data/policy_models/policy_phase2.pt"
  
  # Data paths
  data_dir: "data/teacher_episodes"  # Teacher data directory


# Phase 3: Recurrent RL Training
# -------------------------------
phase3:
  enabled: true
  recurrent: true          # Use recurrent z (previous timestep latent)
  
  # Training settings
  workers: 4               # Number of parallel workers
  total_rounds: 2000
  rounds_per_batch: 2      # Rounds per batch before update
  
  # Model paths
  phase2_model_path: "data/policy_models/policy_phase2.pt"  # Load Phase 2 model
  output_model_path: "agent_code/ppo_agent/ppo_model_phase3.pt"
  
  # Opponent settings
  opponent: "aggressive_teacher_agent"
  selfplay:
    enabled: true
    ratio: 0.2             # Probability of self-play vs opponent
  
  # PPO hyperparameters
  ppo:
    gamma: 0.99            # Discount factor
    gae_lambda: 0.95       # GAE lambda
    clip_range: 0.2        # PPO clip range
    value_coef: 0.5        # Value loss coefficient
    entropy_coef: 0.01     # Entropy coefficient
    max_grad_norm: 0.5     # Gradient clipping
    update_epochs: 4       # Number of update epochs
    batch_size: 64         # Batch size for PPO updates
    learning_rate: 1e-4  # Learning rate


# Training Configuration
# ----------------------
training:
  device: "cuda"           # "cuda" or "cpu" (auto-detect if not specified)
  seed: null               # Random seed (null = random)
  
  # Checkpointing
  save_every: 0            # Save checkpoint every N rounds (0 = only latest)
  checkpoint_dir: "checkpoints"
  
  # Logging
  log_interval: 10         # Log every N epochs/steps
  verbose: true


# Paths and Directories
# ---------------------
paths:
  data_dir: "data"
  teacher_episodes: "data/teacher_episodes"
  env_models: "data/env_models"
  policy_models: "data/policy_models"
  results: "results"
  logs: "logs"


# Environment Variables Mapping
# ------------------------------
# These settings will be exported as environment variables for scripts
env_vars:
  # Model type
  BOMBER_USE_TRM: "1"
  BOMBER_TRM_RECURRENT: "1"
  
  # Model dimensions
  BOMBER_VIT_DIM: "256"
  BOMBER_TRM_Z_DIM: "256"
  
  # TRM hyperparameters
  BOMBER_TRM_N: "4"
  BOMBER_TRM_T: "3"
  BOMBER_TRM_N_SUP: "8"
  BOMBER_TRM_EMA: "0.999"
  
  # Patch settings
  BOMBER_TRM_PATCH_SIZE: "1"
  BOMBER_TRM_PATCH_STRIDE: "1"
  
  # Standard ViT (if not using TRM)
  BOMBER_VIT_DEPTH: "2"
  BOMBER_VIT_HEADS: "4"
  BOMBER_VIT_MIXER: "attn"
  
  # Model path
  PPO_MODEL_PATH: "agent_code/ppo_agent/ppo_model_phase3.pt"


# Presets
# -------
# Quick configuration presets for different scenarios

presets:
  # Small model for fast training
  small:
    model:
      embed_dim: 32
      trm:
        z_dim: 32
        n_latent: 3
        n_sup: 8
    phase2:
      deepsupervision:
        num_epochs: 50
        batch_size: 32
  
  # Medium model (default)
  medium:
    model:
      embed_dim: 64
      trm:
        z_dim: 64
        n_latent: 6
        n_sup: 16
  
  # Large model for better performance
  large:
    model:
      embed_dim: 128
      trm:
        z_dim: 128
        n_latent: 8
        n_sup: 24
    phase2:
      deepsupervision:
        num_epochs: 20
        batch_size: 128
  
  # Overlapping patches for more information
  overlapping_patches:
    model:
      patch:
        size: 2
        stride: 1  # Overlapping patches
  
  # More supervision steps
  high_supervision:
    model:
      trm:
        n_sup: 32
    phase2:
      planning:
        n_planning_steps: 200

